Total 263 number of nodes
<Node method:torch.optim.lr_scheduler.LinearLR.get_lr> - 2
<Node method:torch.optim.sparse_adam.SparseAdam.__init__> - 2
<Node method:torch.optim.optimizer.Optimizer.register_step_post_hook> - 3
<Node method:torch.optim.rmsprop.RMSprop.__setstate__> - 2
<Node class:torch.optim.swa_utils.SWALR> - 1
<Node method:torch.optim.optimizer.Optimizer.zero_grad> - 3
<Node class:torch.optim.lr_scheduler.SequentialLR> - 1
<Node method:torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.step._enable_get_lr_call.__init__> - 3
<Node method:torch.optim.rmsprop.RMSprop.__init__> - 2
<Node function:torch.optim.swa_utils.get_ema_avg_fn.ema_update> - 2
<Node function:torch.optim.optimizer.Optimizer.load_state_dict.update_group> - 4
<Node class:torch.optim.sparse_adam.SparseAdam> - 1
<Node function:torch.optim.swa_utils.get_swa_avg_fn> - 3
<Node method:torch.optim.lr_scheduler.PolynomialLR._get_closed_form_lr> - 2
<Node function:torch.optim.adamax._multi_tensor_adamax> - 4
<Node method:torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.step._enable_get_lr_call.__enter__> - 3
<Node function:torch.optim.optimizer.Optimizer.load_state_dict._cast> - 4
<Node method:torch.optim.optimizer.Optimizer.step> - 3
<Node method:torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.step._enable_get_lr_call.__exit__> - 3
<Node class:torch.optim.rmsprop.RMSprop> - 1
<Node class:torch.optim.lr_scheduler.ChainedScheduler> - 1
<Node method:torch.optim.lr_scheduler.ExponentialLR.__init__> - 2
<Node method:torch.optim.adagrad.Adagrad.step> - 2
<Node method:torch.optim.lr_scheduler.CosineAnnealingLR.get_lr> - 2
<Node class:torch.optim.asgd.ASGD> - 1
<Node method:torch.optim.lr_scheduler.LinearLR.__init__> - 2
<Node class:torch.optim.swa_utils.AveragedModel> - 1
<Node method:torch.optim.rmsprop.RMSprop._init_group> - 2
<Node function:torch.optim.swa_utils.get_swa_avg_fn.swa_update> - 4
<Node function:torch.optim.adadelta._multi_tensor_adadelta> - 4
<Node method:torch.enable_grad.__exit__> - 1
<Node function:torch.optim.swa_utils.update_bn> - 1
<Node function:torch.optim.asgd._to_tensor> - 5
<Node method:torch.optim.rmsprop.RMSprop.step> - 2
<Node method:torch.enable_grad.__enter__> - 1
<Node method:torch.optim.lr_scheduler.MultiplicativeLR.load_state_dict> - 2
<Node function:torch.optim.swa_utils.get_swa_multi_avg_fn> - 3
<Node method:torch.optim.lr_scheduler.CosineAnnealingLR._get_closed_form_lr> - 2
<Node function:torch.optim.swa_utils.get_ema_multi_avg_fn.ema_update> - 2
<Node method:torch.optim.swa_utils.AveragedModel.__init__> - 2
<Node method:torch.optim.lr_scheduler.MultiplicativeLR.state_dict> - 2
<Node method:torch.optim.lr_scheduler.ConstantLR._get_closed_form_lr> - 2
<Node class:torch.optim.lr_scheduler.ExponentialLR> - 1
<Node class:torch.optim.lr_scheduler.ReduceLROnPlateau> - 1
<Node method:torch.optim.lr_scheduler.ChainedScheduler.__init__> - 2
<Node function:torch.optim.lbfgs._cubic_interpolate> - 4
<Node method:torch.optim.adagrad.Adagrad._init_group> - 2
<Node method:torch.optim.swa_utils.AveragedModel.forward> - 2
<Node method:torch.optim.lr_scheduler.PolynomialLR.get_lr> - 2
<Node method:torch.optim.lbfgs.LBFGS._gather_flat_grad> - 2
<Node function:torch.optim.rmsprop._single_tensor_rmsprop> - 4
<Node method:torch.optim.adagrad.Adagrad.share_memory> - 2
<Node method:torch.optim.swa_utils.SWALR.__init__> - 2
<Node method:torch.optim.swa_utils.AveragedModel.update_parameters> - 2
<Node method:torch.optim.lr_scheduler.ConstantLR.get_lr> - 2
<Node method:torch.optim.lbfgs.LBFGS._add_grad> - 2
<Node method:torch.optim.lr_scheduler.MultiplicativeLR.__init__> - 2
<Node method:torch.optim.lr_scheduler.OneCycleLR.get_lr> - 2
<Node method:torch.optim.lbfgs.LBFGS._clone_param> - 2
<Node method:torch.optim.lr_scheduler.ExponentialLR.get_lr> - 2
<Node method:torch.optim.adagrad.Adagrad.__setstate__> - 2
<Node method:torch.optim.lr_scheduler.OneCycleLR._format_param> - 2
<Node class:torch.optim.lr_scheduler.StepLR> - 1
<Node method:torch.optim.lr_scheduler.ChainedScheduler.step> - 2
<Node function:torch.optim.adamw._multi_tensor_adamw> - 4
<Node method:torch.optim.adagrad.Adagrad.__init__> - 2
<Node method:torch.optim.lr_scheduler.OneCycleLR.__init__> - 2
<Node method:torch.optim.lr_scheduler.ChainedScheduler.state_dict> - 2
<Node function:torch.optim.lbfgs._strong_wolfe> - 3
<Node method:torch.optim.optimizer.Optimizer.state_dict> - 3
<Node class:torch.optim.lbfgs.LBFGS> - 1
<Node class:torch.optim.adagrad.Adagrad> - 1
<Node class:torch.optim.adamw.AdamW> - 1
<Node method:torch.optim.lr_scheduler.ExponentialLR._get_closed_form_lr> - 2
<Node class:torch.optim.lr_scheduler.PolynomialLR> - 1
<Node method:torch.optim.lr_scheduler.ConstantLR.__init__> - 2
<Node method:torch.optim.lr_scheduler.SequentialLR.__init__> - 2
<Node function:torch.optim.asgd._multi_tensor_asgd> - 4
<Node function:torch.optim.lr_scheduler._check_verbose_deprecated_warning> - 3
<Node class:torch.optim.radam.RAdam> - 1
<Node class:torch.optim.lr_scheduler.LinearLR> - 1
<Node method:torch.optim.lr_scheduler.MultiStepLR._get_closed_form_lr> - 2
<Node function:torch.optim._functional.sparse_adam> - 3
<Node function:torch.optim.rmsprop._multi_tensor_rmsprop> - 4
<Node method:torch.optim.lbfgs.LBFGS._set_param> - 2
<Node method:torch.optim.radam.RAdam.__init__> - 2
<Node function:torch.optim.adadelta.adadelta> - 3
<Node method:torch.optim.lr_scheduler.ChainedScheduler.load_state_dict> - 2
<Node function:torch.optim.adagrad.adagrad> - 3
<Node method:torch.autograd.__enter__> - 1
<Node function:torch.optim.adagrad._make_sparse> - 5
<Node method:torch.optim.swa_utils.SWALR.get_lr> - 2
<Node function:torch.optim.adam.adam> - 3
<Node method:torch.optim.lr_scheduler.SequentialLR.step> - 2
<Node function:torch.optim.adamw.adamw> - 3
<Node method:torch.optim.lbfgs.LBFGS._directional_evaluate> - 2
<Node method:torch.optim.optimizer.Optimizer.__init__> - 3
<Node method:torch.optim.lbfgs.LBFGS.step> - 2
<Node function:torch.optim.asgd.asgd> - 3
<Node function:torch.optim._functional.sparse_adam.make_sparse> - 4
<Node method:torch.optim.optimizer.Optimizer.register_step_pre_hook> - 3
<Node function:torch.optim.radam.radam> - 3
<Node class:torch.optim.lr_scheduler.CyclicLR> - 1
<Node class:Constant> - 1
<Node function:torch.optim.nadam.nadam> - 3
<Node function:torch.optim.asgd._single_tensor_asgd> - 4
<Node class:torch.optim.adamax.Adamax> - 1
<Node method:torch.optim.lr_scheduler.ReduceLROnPlateau.__init__> - 2
<Node function:torch.optim.radam._multi_tensor_radam> - 4
<Node method:torch.optim.adam.Adam._init_group> - 2
<Node method:torch.optim.lr_scheduler.LRScheduler.state_dict> - 3
<Node class:torch.optim.optimizer.Optimizer> - 2
<Node method:torch.optim.lr_scheduler.ReduceLROnPlateau.step> - 2
<Node function:torch.optim.optimizer.register_optimizer_step_post_hook> - 1
<Node function:torch.optim.optimizer._use_grad_for_differentiable> - 2
<Node function:torch.optim.rmsprop.rmsprop> - 3
<Node function:torch.optim.optimizer._get_value> - 5
<Node method:torch.optim.radam.RAdam._init_group> - 2
<Node function:torch.optim.optimizer._stack_if_compiling> - 5
<Node method:torch.optim.radam.RAdam.__setstate__> - 2
<Node function:torch.optim.sgd.sgd> - 3
<Node function:torch.optim.adamax.adamax> - 3
<Node method:torch.optim.adam.Adam.__setstate__> - 2
<Node function:torch.optim.optimizer._default_to_fused_or_foreach> - 4
<Node method:torch.optim.asgd.ASGD.__init__> - 2
<Node function:torch.optim.optimizer.register_optimizer_step_pre_hook> - 1
<Node method:torch.optim.optimizer.Optimizer.register_state_dict_pre_hook> - 3
<Node function:torch.optim.adamw._fused_adamw> - 4
<Node function:torch.optim.optimizer._view_as_real> - 5
<Node method:torch.optim.asgd.ASGD.__setstate__> - 2
<Node function:torch.optim.adam._multi_tensor_adam> - 4
<Node method:torch.autograd.__exit__> - 1
<Node method:torch.optim.lr_scheduler.LRScheduler.print_lr> - 3
<Node method:torch.optim.lr_scheduler.ReduceLROnPlateau._init_is_better> - 2
<Node method:torch.optim.nadam.NAdam.__init__> - 2
<Node method:torch.optim.lr_scheduler.ReduceLROnPlateau._reset> - 2
<Node method:torch.optim.adam.Adam.step> - 2
<Node method:torch.optim.lr_scheduler.LRScheduler.get_lr> - 3
<Node method:torch.optim.adamax.Adamax.__init__> - 2
<Node function:torch.optim.adam._fused_adam> - 4
<Node function:torch.optim.radam._single_tensor_radam> - 4
<Node class:torch.optim.nadam.NAdam> - 1
<Node method:torch.optim.lr_scheduler.LambdaLR.get_lr> - 2
<Node class:torch.optim.rprop.Rprop> - 1
<Node function:torch.optim.optimizer._dispatch_sqrt> - 5
<Node method:torch.optim.adamax.Adamax.__setstate__> - 2
<Node method:torch.optim.nadam.NAdam.__setstate__> - 2
<Node method:torch.optim.lr_scheduler.LRScheduler.get_last_lr> - 3
<Node method:torch.optim.lr_scheduler.ReduceLROnPlateau.in_cooldown> - 2
<Node method:torch.optim.rprop.Rprop.__setstate__> - 2
<Node function:torch.optim.optimizer._use_grad_for_differentiable._use_grad> - 3
<Node function:torch.optim.rprop.rprop> - 3
<Node method:torch.optim.lbfgs.LBFGS.__init__> - 2
<Node method:torch.optim.adamax.Adamax._init_group> - 2
<Node method:torch.optim.lr_scheduler.LRScheduler.step> - 3
<Node method:torch.optim.lr_scheduler.LambdaLR.__init__> - 2
<Node method:torch.optim.radam.RAdam.step> - 2
<Node function:torch.optim.adam._single_tensor_adam> - 4
<Node class:torch.optim.lr_scheduler.MultiplicativeLR> - 1
<Node method:torch.optim.lr_scheduler.LRScheduler.load_state_dict> - 3
<Node method:torch.optim.lr_scheduler.ReduceLROnPlateau.is_better> - 2
<Node method:torch.optim.lr_scheduler.ReduceLROnPlateau._reduce_lr> - 2
<Node method:torch.optim.nadam.NAdam._init_group> - 2
<Node method:torch.optim.nadam.NAdam.step> - 2
<Node method:torch.optim.adamax.Adamax.step> - 2
<Node method:torch.optim.rprop.Rprop.__init__> - 2
<Node method:torch.optim.lr_scheduler._enable_get_lr_call.__exit__> - 4
<Node method:torch.optim.lr_scheduler.ReduceLROnPlateau.state_dict> - 2
<Node class:torch.optim.sgd.SGD> - 1
<Node function:torch.optim.nadam._single_tensor_nadam> - 4
<Node method:torch.optim.rprop.Rprop._init_group> - 2
<Node function:torch.optim.adamw._single_tensor_adamw> - 4
<Node method:torch.optim.lr_scheduler.LambdaLR.state_dict> - 2
<Node method:torch.optim.sgd.SGD.__init__> - 2
<Node method:torch.optim.rprop.Rprop.step> - 2
<Node method:torch.optim.asgd.ASGD._init_group> - 2
<Node method:torch.optim.optimizer.Optimizer.register_state_dict_post_hook> - 3
<Node method:torch.optim.lr_scheduler.ReduceLROnPlateau.load_state_dict> - 2
<Node method:torch.optim.asgd.ASGD.step> - 2
<Node class:torch.optim.lr_scheduler.CosineAnnealingWarmRestarts> - 1
<Node method:torch.optim.lr_scheduler.LambdaLR.load_state_dict> - 2
<Node method:torch.optim.adam.Adam.__init__> - 2
<Node method:torch.optim.sgd.SGD.__setstate__> - 2
<Node method:torch.optim.lr_scheduler._enable_get_lr_call.__enter__> - 4
<Node method:torch.optim.lr_scheduler.CyclicLR.__init__> - 2
<Node class:torch.optim.lr_scheduler._enable_get_lr_call> - 4
<Node function:torch.optim.swa_utils.get_ema_avg_fn> - 1
<Node method:torch.optim.lr_scheduler._enable_get_lr_call.__init__> - 4
<Node function:torch.optim.adamax._single_tensor_adamax> - 4
<Node function:torch.optim.rprop._single_tensor_rprop> - 4
<Node function:torch.optim.swa_utils.get_swa_multi_avg_fn.swa_update> - 4
<Node class:torch.optim.lr_scheduler.LambdaLR> - 1
<Node function:torch.optim.nadam._multi_tensor_nadam> - 4
<Node function:torch.optim.adagrad._multi_tensor_adagrad> - 4
<Node method:torch.optim.lr_scheduler.CyclicLR._format_param> - 2
<Node method:torch.optim.lr_scheduler.CyclicLR._init_scale_fn> - 2
<Node class:torch.optim.lr_scheduler.CosineAnnealingLR> - 1
<Node method:torch.optim.lr_scheduler.CyclicLR.scale_fn> - 2
<Node method:torch.optim.lr_scheduler.PolynomialLR.__init__> - 2
<Node function:torch.optim.adagrad._single_tensor_adagrad> - 4
<Node function:torch.optim._multi_tensor.partialclass> - 1
<Node method:torch.optim.optimizer.Optimizer.__getstate__> - 3
<Node class:torch.optim.lr_scheduler.LRScheduler> - 2
<Node method:torch.optim.lr_scheduler.CyclicLR.get_lr> - 2
<Node class:torch.optim._multi_tensor.partialclass.NewCls> - 2
<Node method:torch.optim.optimizer.Optimizer.__setstate__> - 3
<Node method:torch.optim.optimizer.Optimizer._patch_step_function> - 3
<Node method:torch.optim.lr_scheduler.SequentialLR.load_state_dict> - 2
<Node method:torch.optim.lbfgs.LBFGS._numel> - 2
<Node function:torch.optim.swa_utils.get_ema_multi_avg_fn> - 1
<Node class:torch.optim.optimizer._RequiredParameter> - 4
<Node method:torch.optim.optimizer.Optimizer.add_param_group> - 3
<Node function:torch.optim.rprop._multi_tensor_rprop> - 4
<Node class:torch.optim.adadelta.Adadelta> - 1
<Node method:torch.optim.optimizer.Optimizer.load_state_dict> - 3
<Node function:torch.optim.lr_scheduler.LRScheduler.__init__.with_counter> - 4
<Node class:torch.optim.lr_scheduler.OneCycleLR> - 1
<Node method:torch.optim.optimizer.Optimizer.__repr__> - 3
<Node method:torch.optim.lr_scheduler.SequentialLR.state_dict> - 2
<Node method:torch.optim.adadelta.Adadelta.__init__> - 2
<Node method:torch.optim.optimizer._RequiredParameter.__repr__> - 5
<Node method:torch.optim.lr_scheduler.CyclicLR.state_dict> - 2
<Node method:torch.optim.lr_scheduler.CyclicLR.load_state_dict> - 2
<Node method:torch.optim.optimizer.Optimizer._cuda_graph_capture_health_check> - 3
<Node method:torch.optim.adadelta.Adadelta.__setstate__> - 2
<Node method:torch.optim.lr_scheduler.MultiplicativeLR.get_lr> - 2
<Node method:torch.optim.adamw.AdamW.step> - 2
<Node class:torch.optim.lr_scheduler.MultiStepLR> - 1
<Node method:torch.optim.adamw.AdamW._init_group> - 2
<Node method:torch.optim.lr_scheduler.StepLR.__init__> - 2
<Node method:torch.optim.optimizer.Optimizer._optimizer_step_code> - 3
<Node method:torch.optim.adadelta.Adadelta._init_group> - 2
<Node method:torch.optim.optimizer._RequiredParameter.__init__> - 1
<Node function:torch.optim.sgd._multi_tensor_sgd> - 4
<Node method:torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.__init__> - 2
<Node function:torch.optim.adadelta._single_tensor_adadelta> - 4
<Node method:torch.optim.lr_scheduler.LRScheduler._initial_step> - 3
<Node method:torch.optim.adadelta.Adadelta.step> - 2
<Node method:torch.optim.lr_scheduler.MultiStepLR.get_lr> - 2
<Node method:torch.optim.optimizer.Optimizer.register_load_state_dict_post_hook> - 3
<Node method:torch.optim.lr_scheduler.StepLR.get_lr> - 2
<Node function:torch.optim.lr_scheduler.LRScheduler.__init__.with_counter.wrapper> - 5
<Node method:torch.optim.optimizer.Optimizer.register_load_state_dict_pre_hook> - 3
<Node method:torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.get_lr> - 2
<Node class:torch.optim.lr_scheduler._LRScheduler> - 1
<Node method:torch.optim.lr_scheduler.CosineAnnealingLR.__init__> - 2
<Node function:torch.optim.optimizer.Optimizer.state_dict.pack_group> - 4
<Node function:torch.optim.sgd._single_tensor_sgd> - 4
<Node method:torch.optim.lr_scheduler.LRScheduler.__init__> - 3
<Node method:torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.step> - 2
<Node function:torch.optim.optimizer.Optimizer.profile_hook_step.wrapper> - 1
<Node function:torch.optim.lbfgs.LBFGS.step.obj_func> - 3
<Node class:torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.step._enable_get_lr_call> - 3
<Node method:torch.optim.lr_scheduler.StepLR._get_closed_form_lr> - 2
<Node method:torch.optim.adamw.AdamW.__init__> - 2
<Node class:torch.optim.lr_scheduler.ConstantLR> - 1
<Node method:torch.optim.lr_scheduler.MultiStepLR.__init__> - 2
<Node method:torch.optim.sgd.SGD.step> - 2
<Node method:torch.optim.lr_scheduler.LinearLR._get_closed_form_lr> - 2
<Node class:torch.optim.adam.Adam> - 1
<Node method:torch.optim.sgd.SGD._init_group> - 2
<Node method:torch.optim.adamw.AdamW.__setstate__> - 2
<Node method:torch.optim.sparse_adam.SparseAdam.step> - 2
