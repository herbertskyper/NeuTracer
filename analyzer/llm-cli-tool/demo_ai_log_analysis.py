#!/usr/bin/env python3
"""
AIæ¨¡å‹æ—¥å¿—åˆ†æåŠŸèƒ½æ¼”ç¤ºè„šæœ¬
å±•ç¤ºå¦‚ä½•ä½¿ç”¨LLMç³»ç»Ÿæç¤ºè¾…åŠ©è¯Šæ–­AIè®­ç»ƒå’Œæ¨ç†æ—¥å¿—
"""

import sys
import os

sys.path.append("src")

from model_log_analyzer import ModelLogAnalyzer


def demo_basic_analysis():
    """æ¼”ç¤ºåŸºç¡€æ—¥å¿—åˆ†æåŠŸèƒ½"""
    print("=" * 60)
    print("ğŸ” åŸºç¡€AIæ¨¡å‹æ—¥å¿—åˆ†ææ¼”ç¤º")
    print("=" * 60)

    analyzer = ModelLogAnalyzer()

    # åˆ†æè®­ç»ƒæ—¥å¿—
    print("\nğŸ“š åˆ†æè®­ç»ƒæ—¥å¿—...")
    training_result = analyzer.analyze_log_file("example_training_log.txt")

    if "error" not in training_result:
        summary = training_result.get("summary", {})
        anomalies = training_result.get("anomalies", [])

        print(f"âœ… æ£€æµ‹åˆ° {len(anomalies)} ä¸ªå¼‚å¸¸")
        print(
            f"ğŸ“Š æå–äº† {sum(len(metrics) for metrics in training_result.get('metrics', {}).values())} ä¸ªæŒ‡æ ‡"
        )

        # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
        for metric_name, metric_data in summary.items():
            if metric_name != "anomalies" and isinstance(metric_data, dict):
                if metric_data.get("count", 0) > 0:
                    print(
                        f"   â€¢ {metric_name}: {metric_data['count']}ä¸ªæ•°æ®ç‚¹, è¶‹åŠ¿: {metric_data.get('trend', 'unknown')}"
                    )


def demo_llm_system_prompt():
    """æ¼”ç¤ºLLMç³»ç»Ÿæç¤ºç”ŸæˆåŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("ğŸ¤– LLMç³»ç»Ÿæç¤ºç”Ÿæˆæ¼”ç¤º")
    print("=" * 60)

    analyzer = ModelLogAnalyzer()

    # ä½¿ç”¨LLMè¾…åŠ©åˆ†æ
    result = analyzer.analyze_with_llm_assistance(
        "example_training_log.txt", "è®­ç»ƒè¿‡ç¨‹ä¸­GPUå†…å­˜ä½¿ç”¨æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ"
    )

    if "llm_analysis" in result:
        llm_data = result["llm_analysis"]
        system_prompt = llm_data.get("system_prompt", "")

        print("ğŸ“ ç”Ÿæˆçš„ç³»ç»Ÿæç¤ºé¢„è§ˆ (å‰800å­—ç¬¦):")
        print("-" * 40)
        print(
            system_prompt[:800] + "..." if len(system_prompt) > 800 else system_prompt
        )

        print(f"\nğŸ“ å®Œæ•´ç³»ç»Ÿæç¤ºé•¿åº¦: {len(system_prompt)} å­—ç¬¦")
        print(f"ğŸ¯ ç›®æ ‡æ¨¡å‹: {llm_data.get('model', 'deepseek-chat')}")
        print(f"âœ… å‡†å¤‡å°±ç»ª: {llm_data.get('ready_for_llm', False)}")


def demo_anomaly_detection():
    """æ¼”ç¤ºå¼‚å¸¸æ£€æµ‹åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("âš ï¸ å¼‚å¸¸æ£€æµ‹åŠŸèƒ½æ¼”ç¤º")
    print("=" * 60)

    analyzer = ModelLogAnalyzer()

    # åˆ†ææ¨ç†æ—¥å¿—
    inference_result = analyzer.analyze_log_file("example_inference_log.txt")

    if "error" not in inference_result:
        anomalies = inference_result.get("anomalies", [])

        print(f"ğŸ” åœ¨æ¨ç†æ—¥å¿—ä¸­æ£€æµ‹åˆ° {len(anomalies)} ä¸ªå¼‚å¸¸:")

        # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„æ˜¾ç¤º
        severity_groups = {}
        for anomaly in anomalies:
            severity = anomaly.get("severity", "unknown")
            if severity not in severity_groups:
                severity_groups[severity] = []
            severity_groups[severity].append(anomaly)

        severity_icons = {"critical": "ğŸ”´", "high": "ğŸŸ ", "medium": "ğŸŸ¡", "low": "ğŸŸ¢"}

        for severity, anomaly_list in severity_groups.items():
            icon = severity_icons.get(severity, "âšª")
            print(f"\n{icon} {severity.upper()} çº§å¼‚å¸¸ ({len(anomaly_list)}ä¸ª):")
            for i, anomaly in enumerate(anomaly_list[:3], 1):  # æœ€å¤šæ˜¾ç¤º3ä¸ª
                print(
                    f"   {i}. {anomaly.get('description', 'N/A')} (è¡Œ {anomaly.get('line_number', 'N/A')})"
                )
                print(f"      ç±»åˆ«: {anomaly.get('category', 'N/A')}")
                if anomaly.get("metric_value") is not None:
                    print(f"      å€¼: {anomaly.get('metric_value')}")


def demo_metrics_extraction():
    """æ¼”ç¤ºæŒ‡æ ‡æå–åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("ğŸ“Š æŒ‡æ ‡æå–åŠŸèƒ½æ¼”ç¤º")
    print("=" * 60)

    analyzer = ModelLogAnalyzer()

    # åˆ†æè®­ç»ƒæ—¥å¿—çš„æŒ‡æ ‡
    result = analyzer.analyze_log_file("example_training_log.txt")

    if "error" not in result:
        metrics = result.get("metrics", {})

        print("ğŸ“ˆ æå–çš„å…³é”®æŒ‡æ ‡ç±»å‹:")

        for metric_name, metric_list in metrics.items():
            if metric_list:  # åªæ˜¾ç¤ºæœ‰æ•°æ®çš„æŒ‡æ ‡
                first_metric = metric_list[0]
                last_metric = metric_list[-1]

                print(f"\nâ€¢ {metric_name}:")
                print(f"  æ•°æ®ç‚¹æ•°é‡: {len(metric_list)}")
                print(
                    f"  é¦–æ¬¡è®°å½•: {first_metric.get('value')} (è¡Œ {first_metric.get('line_number')})"
                )
                print(
                    f"  æœ€æ–°è®°å½•: {last_metric.get('value')} (è¡Œ {last_metric.get('line_number')})"
                )

                # æ˜¾ç¤ºå€¼åŸŸ
                values = [m.get("value", 0) for m in metric_list]
                print(f"  å€¼åŸŸ: {min(values):.4f} - {max(values):.4f}")


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸš€ AIæ¨¡å‹æ—¥å¿—åˆ†æå·¥å…·æ¼”ç¤º")
    print("æ”¯æŒä½¿ç”¨LLMç³»ç»Ÿæç¤ºè¿›è¡Œæ™ºèƒ½è¯Šæ–­")

    # æ£€æŸ¥ç¤ºä¾‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    required_files = ["example_training_log.txt", "example_inference_log.txt"]
    missing_files = [f for f in required_files if not os.path.exists(f)]

    if missing_files:
        print(f"\nâŒ ç¼ºå°‘ç¤ºä¾‹æ–‡ä»¶: {', '.join(missing_files)}")
        print("è¯·ç¡®ä¿ç¤ºä¾‹æ—¥å¿—æ–‡ä»¶å­˜åœ¨äºå½“å‰ç›®å½•")
        return

    try:
        # è¿è¡Œå„ç§æ¼”ç¤º
        demo_basic_analysis()
        demo_anomaly_detection()
        demo_metrics_extraction()
        demo_llm_system_prompt()

        print("\n" + "=" * 60)
        print("ğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
        print("=" * 60)
        print("\nğŸ’¡ ä½¿ç”¨æç¤º:")
        print("1. åŸºç¡€åˆ†æ: python -m src.cli --analyze-model-log your_log.txt")
        print(
            "2. LLMè¾…åŠ©åˆ†æ: python -m src.cli --analyze-model-log your_log.txt --use-llm-analysis"
        )
        print(
            "3. ç‰¹å®šé—®é¢˜åˆ†æ: python -m src.cli --analyze-model-log your_log.txt --model-log-question 'ä½ çš„é—®é¢˜'"
        )
        print(
            "4. å®Œæ•´LLMåˆ†æ: python -m src.cli --analyze-model-log your_log.txt --use-llm-analysis --model-log-question 'è¯¦ç»†é—®é¢˜'"
        )

    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
