import argparse
import sys
import os
from .api import send_request
from .config import DEFAULT_MODEL, set_api_key
from .settings import save_api_key, get_saved_api_key
from .reasoning_logger import ReasoningLogger, prepare_file_analysis_prompt
from .model_log_analyzer import ModelLogAnalyzer


def main():
    parser = argparse.ArgumentParser(
        description="Command-line tool for interacting with the LLM API."
    )
    parser.add_argument(
        "content", type=str, nargs="?", help="The content input for the model."
    )
    parser.add_argument(
        "-m",
        type=str,
        default=DEFAULT_MODEL,
        help=f"The model to use for the request. (default: {DEFAULT_MODEL})",
    )
    parser.add_argument(
        "--no-stream",
        action="store_true",
        help="Disable streaming response (default is to stream tokens)",
    )
    parser.add_argument(
        "--api-key",
        type=str,
        help="SiliconFlow API key. Can also be set via SILICONFLOW_API_KEY environment variable.",
    )
    parser.add_argument(
        "--save-key",
        action="store_true",
        help="Save the API key to use in future sessions.",
    )

    # New reasoning and file analysis arguments
    parser.add_argument(
        "--file",
        type=str,
        help="Path to a file to analyze. If provided, the AI will analyze this file.",
    )
    parser.add_argument(
        "--reasoning",
        action="store_true",
        help="Enable detailed reasoning process logging.",
    )
    parser.add_argument(
        "--log-dir",
        type=str,
        help="Directory to store reasoning logs (default: ~/.llm-cli-tool/reasoning_logs)",
    )
    parser.add_argument(
        "--analyze-logs",
        type=str,
        metavar="SESSION_ID",
        help="Analyze reasoning logs for a specific session ID.",
    )
    parser.add_argument(
        "--list-sessions",
        action="store_true",
        help="List all available reasoning sessions.",
    )

    # AIæ¨¡å‹æ—¥å¿—åˆ†æå‚æ•°
    parser.add_argument(
        "--analyze-model-log",
        type=str,
        metavar="LOG_FILE",
        help="æ™ºèƒ½åˆ†æAIè®­ç»ƒ/æ¨ç†æ—¥å¿—æ–‡ä»¶ï¼Œè¯†åˆ«å¼‚å¸¸å’Œæ€§èƒ½é—®é¢˜",
    )
    parser.add_argument(
        "--model-log-question",
        type=str,
        help="é’ˆå¯¹AIæ¨¡å‹æ—¥å¿—çš„å…·ä½“åˆ†æé—®é¢˜æˆ–å…³æ³¨ç‚¹",
    )
    parser.add_argument(
        "--use-llm-analysis",
        action="store_true",
        help="ä½¿ç”¨LLMæ™ºèƒ½è¾…åŠ©åˆ†ææ—¥å¿—ï¼ˆéœ€è¦APIå¯†é’¥ï¼‰",
    )

    args = parser.parse_args()

    # Initialize reasoning logger if needed
    logger = None
    if (
        args.reasoning
        or args.analyze_logs
        or args.list_sessions
        or args.analyze_model_log
    ):
        logger = ReasoningLogger(args.log_dir)

    # Handle AI model log analysis
    if args.analyze_model_log:
        handle_model_log_analysis(args, logger)
        return

    # Handle log analysis and session listing
    if args.analyze_logs:
        analysis = logger.analyze_session(args.analyze_logs)
        print(f"Session Analysis for {args.analyze_logs}:")
        print("-" * 50)
        for key, value in analysis.items():
            print(f"{key}: {value}")
        return

    if args.list_sessions:
        sessions = logger.list_sessions()
        if sessions:
            print("Available reasoning sessions:")
            print("-" * 30)
            for session in sessions:
                print(f"  {session}")
        else:
            print("No reasoning sessions found.")
        return

    # Handle file analysis
    if args.file:
        if not os.path.exists(args.file):
            print(f"Error: File '{args.file}' not found.", file=sys.stderr)
            sys.exit(1)

        # Prepare file analysis prompt
        file_prompt = prepare_file_analysis_prompt(args.file, args.content)
        args.content = file_prompt

        if logger:
            logger.start_new_session(
                {
                    "type": "file_analysis",
                    "file_path": args.file,
                    "original_question": args.content,
                }
            )
            logger.log_reasoning_step("file_read", f"Reading file: {args.file}")
    elif not args.content:
        print("Error: Content is required unless using --file option.", file=sys.stderr)
        parser.print_help()
        sys.exit(1)

    # Process API key
    saved_key = get_saved_api_key()
    api_key = (
        args.api_key
        if args.api_key is not None
        else (
            os.environ.get("SILICONFLOW_API_KEY")
            if os.environ.get("SILICONFLOW_API_KEY")
            else saved_key
        )
    )

    if not api_key:
        print(
            "Error: API key not provided. Use --api-key option, set SILICONFLOW_API_KEY environment variable, or save a key with --save-key.",
            file=sys.stderr,
        )
        sys.exit(1)

    # Save the API key if requested and different from the saved one
    if args.save_key and args.api_key and args.api_key != saved_key:
        if save_api_key(args.api_key):
            print("API key saved successfully.")
        else:
            print("Warning: Failed to save API key.", file=sys.stderr)

    # Set the API key for this session
    set_api_key(api_key)

    stream = not args.no_stream

    if logger:
        logger.log_reasoning_step(
            "input", args.content, {"model": args.m, "stream": stream}
        )

    if stream:
        handle_streaming_response(args.content, args.m, logger)
    else:
        handle_normal_response(args.content, args.m, logger)

    if logger:
        logger.end_session("Reasoning session completed successfully")


def handle_streaming_response(content, model, logger=None):
    """Handle streaming responses, printing tokens as they arrive."""
    sys.stdout.write("Response: ")
    sys.stdout.flush()

    if logger:
        logger.log_reasoning_step("streaming_start", "Starting streaming response")

    full_response = ""
    for chunk in send_request(content, model, stream=True):
        if "error" in chunk:
            if logger:
                logger.log_reasoning_step("error", f"Chunk error: {chunk['error']}")
            print(f"\nChunkError: {chunk['error']}", file=sys.stderr)
            sys.exit(1)

        if "content" in chunk:
            if chunk["content"] is not None:
                sys.stdout.write(chunk["content"])
                sys.stdout.flush()
                full_response += chunk["content"]

    # Add a newline at the end
    sys.stdout.write("\n")
    sys.stdout.flush()

    if logger:
        logger.log_reasoning_step("response_complete", full_response)


def handle_normal_response(content, model, logger=None):
    """Handle non-streaming responses."""
    if logger:
        logger.log_reasoning_step("request_start", "Sending non-streaming request")

    response = send_request(content, model, stream=False)

    if "error" in response:
        if logger:
            logger.log_reasoning_step("error", f"Response error: {response['error']}")
        print(f"Error: {response['error']}", file=sys.stderr)
        sys.exit(1)

    if "choices" in response and response["choices"]:
        for i, choice in enumerate(response["choices"]):
            if "message" in choice and "content" in choice["message"]:
                response_content = choice["message"]["content"]
                print(f"Response {i+1}:\n{response_content}")
                if logger:
                    logger.log_reasoning_step(
                        "response_complete", response_content, {"choice_index": i}
                    )
    else:
        print("No valid response received.")
        if logger:
            logger.log_reasoning_step("error", "No valid response received")


def handle_model_log_analysis(args, logger=None):
    """å¤„ç†AIæ¨¡å‹æ—¥å¿—åˆ†æ"""
    if not os.path.exists(args.analyze_model_log):
        print(f"é”™è¯¯: æ—¥å¿—æ–‡ä»¶ '{args.analyze_model_log}' ä¸å­˜åœ¨.", file=sys.stderr)
        sys.exit(1)

    print(f"æ­£åœ¨åˆ†æAIæ¨¡å‹æ—¥å¿—æ–‡ä»¶: {args.analyze_model_log}")
    print("-" * 60)

    # åˆ›å»ºæ¨¡å‹æ—¥å¿—åˆ†æå™¨
    analyzer = ModelLogAnalyzer()

    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨LLMè¾…åŠ©åˆ†æ
    if args.use_llm_analysis:
        # ä½¿ç”¨LLMè¾…åŠ©åˆ†æ
        analysis_result = analyzer.analyze_with_llm_assistance(
            args.analyze_model_log, args.model_log_question
        )

        if "error" in analysis_result:
            print(f"åˆ†æå¤±è´¥: {analysis_result['error']}", file=sys.stderr)
            sys.exit(1)

        # æ˜¾ç¤ºåŸºç¡€åˆ†æç»“æœ
        print_model_log_analysis(analysis_result, args.model_log_question)

        # æ£€æŸ¥APIå¯†é’¥å’Œè¿›è¡ŒLLMåˆ†æ
        saved_key = get_saved_api_key()
        api_key = os.environ.get("SILICONFLOW_API_KEY", saved_key)

        if not api_key:
            print("\n" + "=" * 60)
            print("ğŸ¤– LLMæ™ºèƒ½åˆ†æ")
            print("=" * 60)
            print("âŒ æœªæ‰¾åˆ°APIå¯†é’¥ï¼Œæ— æ³•è¿›è¡ŒLLMæ™ºèƒ½åˆ†æ")
            print("è¯·è®¾ç½®SILICONFLOW_API_KEYç¯å¢ƒå˜é‡æˆ–ä½¿ç”¨--save-keyä¿å­˜å¯†é’¥")
            print("\nğŸ“‹ ç³»ç»Ÿæç¤ºé¢„è§ˆ:")
            print("-" * 40)
            llm_data = analysis_result.get("llm_analysis", {})
            system_prompt = llm_data.get("system_prompt", "")
            # æ˜¾ç¤ºç³»ç»Ÿæç¤ºçš„å‰500å­—ç¬¦
            print(
                system_prompt[:500] + "..."
                if len(system_prompt) > 500
                else system_prompt
            )
        else:
            # è®¾ç½®APIå¯†é’¥å¹¶è¿›è¡ŒLLMåˆ†æ
            set_api_key(api_key)

            print("\n" + "=" * 60)
            print("ğŸ¤– LLMæ™ºèƒ½åˆ†æä¸­...")
            print("=" * 60)

            llm_data = analysis_result.get("llm_analysis", {})
            system_prompt = llm_data.get("system_prompt", "")

            # æ„å»ºå®Œæ•´çš„åˆ†ææç¤º
            full_prompt = (
                f"è¯·åŸºäºä»¥ä¸‹AIæ¨¡å‹æ—¥å¿—åˆ†ææ•°æ®æä¾›ä¸“ä¸šè¯Šæ–­:\n\n{system_prompt}"
            )

            # è¿›è¡ŒLLMåˆ†æ
            try:
                print("ğŸ“¤ æ­£åœ¨å‘é€åˆ†æè¯·æ±‚åˆ°LLM...")
                response = send_request(full_prompt, args.m, stream=False)

                if "error" in response:
                    print(f"âŒ LLMåˆ†æå¤±è´¥: {response['error']}", file=sys.stderr)
                else:
                    print("âœ… LLMåˆ†æå®Œæˆ:")
                    print("-" * 40)
                    if "choices" in response and response["choices"]:
                        llm_response = response["choices"][0]["message"]["content"]
                        print(llm_response)

                        # ä¿å­˜LLMåˆ†æç»“æœ
                        analysis_result["llm_analysis"]["response"] = llm_response
                    else:
                        print("âŒ æœªæ”¶åˆ°æœ‰æ•ˆçš„LLMå“åº”")

            except Exception as e:
                print(f"âŒ LLMåˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}", file=sys.stderr)
    else:
        # åŸºç¡€åˆ†æ
        analysis_result = analyzer.analyze_log_file(args.analyze_model_log)

        if "error" in analysis_result:
            print(f"åˆ†æå¤±è´¥: {analysis_result['error']}", file=sys.stderr)
            sys.exit(1)

        # æ˜¾ç¤ºåˆ†æç»“æœ
        print_model_log_analysis(analysis_result, args.model_log_question)

    # å¦‚æœå¯ç”¨äº†æ¨ç†æ—¥å¿—è®°å½•
    if logger:
        logger.start_new_session(
            {
                "type": "model_log_analysis",
                "log_file": args.analyze_model_log,
                "question": args.model_log_question,
                "use_llm": args.use_llm_analysis,
            }
        )
        logger.log_reasoning_step("analysis_complete", analysis_result)
        logger.end_session("Model log analysis completed")


def print_model_log_analysis(analysis_result, specific_question=None):
    """æ‰“å°æ¨¡å‹æ—¥å¿—åˆ†æç»“æœ"""
    summary = analysis_result.get("summary", {})
    anomalies = analysis_result.get("anomalies", [])
    recommendations = analysis_result.get("recommendations", [])

    # åŸºæœ¬ä¿¡æ¯
    print(f"ğŸ“„ æ–‡ä»¶: {analysis_result.get('file_path', 'N/A')}")
    print(f"ğŸ“Š æ€»è¡Œæ•°: {analysis_result.get('total_lines', 0)}")
    print(f"â° åˆ†ææ—¶é—´: {analysis_result.get('analysis_time', 'N/A')}")
    print()

    # å¼‚å¸¸ç»Ÿè®¡
    anomaly_summary = summary.get("anomalies", {})
    print("ğŸš¨ å¼‚å¸¸ç»Ÿè®¡:")
    print(f"  æ€»è®¡: {anomaly_summary.get('total', 0)}")

    if "by_severity" in anomaly_summary:
        print("  æŒ‰ä¸¥é‡ç¨‹åº¦:")
        for severity, count in anomaly_summary["by_severity"].items():
            emoji = {"critical": "ğŸ”´", "high": "ğŸŸ ", "medium": "ğŸŸ¡", "low": "ğŸŸ¢"}.get(
                severity, "âšª"
            )
            print(f"    {emoji} {severity}: {count}")

    if "by_category" in anomaly_summary:
        print("  æŒ‰ç±»åˆ«:")
        for category, count in anomaly_summary["by_category"].items():
            print(f"    â€¢ {category}: {count}")
    print()

    # æŒ‡æ ‡æ‘˜è¦
    print("ğŸ“ˆ å…³é”®æŒ‡æ ‡æ‘˜è¦:")
    metrics_displayed = 0
    for metric_name, metric_data in summary.items():
        if metric_name == "anomalies" or not isinstance(metric_data, dict):
            continue
        if "count" in metric_data and metric_data["count"] > 0:
            print(f"  {metric_name}:")
            print(f"    æ•°é‡: {metric_data['count']}")
            print(f"    èŒƒå›´: {metric_data['min']:.4f} - {metric_data['max']:.4f}")
            print(f"    å¹³å‡: {metric_data['avg']:.4f}")
            print(f"    æœ€æ–°: {metric_data['last']:.4f}")
            print(f"    è¶‹åŠ¿: {metric_data.get('trend', 'unknown')}")
            metrics_displayed += 1
            if metrics_displayed >= 5:  # åªæ˜¾ç¤ºå‰5ä¸ªæŒ‡æ ‡
                break

    if metrics_displayed == 0:
        print("  æœªæ£€æµ‹åˆ°å…³é”®æŒ‡æ ‡")
    print()

    # é‡è¦å¼‚å¸¸è¯¦æƒ…
    if anomalies:
        print("âš ï¸  é‡è¦å¼‚å¸¸è¯¦æƒ…:")
        critical_anomalies = [a for a in anomalies if a.get("severity") == "critical"]
        high_anomalies = [a for a in anomalies if a.get("severity") == "high"]

        # æ˜¾ç¤ºå…³é”®å¼‚å¸¸
        for anomaly in critical_anomalies[:3]:  # æœ€å¤šæ˜¾ç¤º3ä¸ªå…³é”®å¼‚å¸¸
            print(f"  ğŸ”´ ä¸¥é‡: {anomaly.get('description', 'N/A')}")
            print(f"     è¡Œå·: {anomaly.get('line_number', 'N/A')}")
            print(f"     å»ºè®®: {anomaly.get('suggestion', 'N/A')}")
            print()

        # æ˜¾ç¤ºé«˜çº§å¼‚å¸¸
        for anomaly in high_anomalies[:2]:  # æœ€å¤šæ˜¾ç¤º2ä¸ªé«˜çº§å¼‚å¸¸
            print(f"  ğŸŸ  é«˜çº§: {anomaly.get('description', 'N/A')}")
            print(f"     è¡Œå·: {anomaly.get('line_number', 'N/A')}")
            print(f"     å»ºè®®: {anomaly.get('suggestion', 'N/A')}")
            print()

    # æ¨èæ“ä½œ
    if recommendations:
        print("ğŸ’¡ æ¨èæ“ä½œ:")
        for i, rec in enumerate(recommendations[:5], 1):  # æœ€å¤šæ˜¾ç¤º5ä¸ªå»ºè®®
            print(f"  {i}. {rec}")
        print()

    # å¦‚æœæœ‰ç‰¹å®šé—®é¢˜ï¼Œæä¾›é’ˆå¯¹æ€§åˆ†æ
    if specific_question:
        print(f"ğŸ¯ é’ˆå¯¹é—®é¢˜ '{specific_question}' çš„åˆ†æ:")
        print("  (åŸºäºæ£€æµ‹åˆ°çš„å¼‚å¸¸å’ŒæŒ‡æ ‡)")

        # æ ¹æ®é—®é¢˜å…³é”®è¯åŒ¹é…ç›¸å…³å¼‚å¸¸å’ŒæŒ‡æ ‡
        question_lower = specific_question.lower()
        relevant_anomalies = []
        relevant_metrics = []

        # åŒ¹é…å¼‚å¸¸
        for anomaly in anomalies:
            category = anomaly.get("category", "").lower()
            if any(
                keyword in question_lower
                for keyword in ["æ€§èƒ½", "å»¶è¿Ÿ", "latency", "throughput", "åå"]
            ) and any(
                keyword in category
                for keyword in ["performance", "gpu", "throughput", "latency"]
            ):
                relevant_anomalies.append(anomaly)
            elif any(
                keyword in question_lower
                for keyword in ["è®­ç»ƒ", "training", "é—®é¢˜", "error", "é”™è¯¯"]
            ) and any(
                keyword in category
                for keyword in ["training", "memory", "gpu", "convergence"]
            ):
                relevant_anomalies.append(anomaly)
            elif any(
                keyword in question_lower for keyword in ["å†…å­˜", "memory", "gpu"]
            ) and any(keyword in category for keyword in ["memory", "gpu"]):
                relevant_anomalies.append(anomaly)

        # åŒ¹é…æŒ‡æ ‡è¶‹åŠ¿
        for metric_name, metric_data in summary.items():
            if metric_name == "anomalies" or not isinstance(metric_data, dict):
                continue
            if any(
                keyword in question_lower for keyword in ["æ€§èƒ½", "åå", "throughput"]
            ) and metric_name in ["throughput", "latency"]:
                relevant_metrics.append((metric_name, metric_data))
            elif any(
                keyword in question_lower for keyword in ["è®­ç»ƒ", "training"]
            ) and metric_name in ["loss", "accuracy", "learning_rate"]:
                relevant_metrics.append((metric_name, metric_data))

        if relevant_anomalies:
            print("  ç›¸å…³å¼‚å¸¸:")
            for anomaly in relevant_anomalies[:3]:
                severity_emoji = {
                    "critical": "ğŸ”´",
                    "high": "ğŸŸ ",
                    "medium": "ğŸŸ¡",
                    "low": "ğŸŸ¢",
                }.get(anomaly.get("severity", "low"), "âšª")
                print(
                    f"    {severity_emoji} {anomaly.get('description', 'N/A')} (è¡Œ {anomaly.get('line_number', 'N/A')})"
                )

        if relevant_metrics:
            print("  ç›¸å…³æŒ‡æ ‡è¶‹åŠ¿:")
            for metric_name, metric_data in relevant_metrics[:3]:
                trend = metric_data.get("trend", "stable")
                trend_emoji = {
                    "increasing": "ğŸ“ˆ",
                    "decreasing": "ğŸ“‰",
                    "stable": "â¡ï¸",
                }.get(trend, "â¡ï¸")
                print(
                    f"    {trend_emoji} {metric_name}: {trend} (å½“å‰: {metric_data.get('last', 'N/A')})"
                )

        if not relevant_anomalies and not relevant_metrics:
            print("  æœªå‘ç°ä¸è¯¥é—®é¢˜ç›´æ¥ç›¸å…³çš„å¼‚å¸¸æˆ–å…³é”®æŒ‡æ ‡å˜åŒ–")
        print()

    print("=" * 60)


if __name__ == "__main__":
    main()
