"""
AIæ¨¡å‹æ—¥å¿—åˆ†æå™¨
ä¸“é—¨ç”¨äºåˆ†æAIè®­ç»ƒå’Œæ¨ç†æ—¥å¿—ï¼Œè¯†åˆ«å¼‚å¸¸æŒ‡æ ‡å’Œé—®é¢˜
"""

import re
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass


@dataclass
class LogMetric:
    """æ—¥å¿—æŒ‡æ ‡æ•°æ®ç»“æ„"""

    name: str
    value: float
    timestamp: str
    line_number: int
    context: str


@dataclass
class LogAnomaly:
    """æ—¥å¿—å¼‚å¸¸æ•°æ®ç»“æ„"""

    category: str  # memory, gpu, training, inference, model, config
    severity: str  # critical, high, medium, low
    description: str
    line_number: int
    metric_value: Optional[float]
    threshold: Optional[float]
    suggestion: str
    context: str


class ModelLogAnalyzer:
    """AIæ¨¡å‹æ—¥å¿—åˆ†æå™¨"""

    def __init__(self):
        self.metrics_patterns = self._init_metrics_patterns()
        self.anomaly_patterns = self._init_anomaly_patterns()
        self.thresholds = self._init_thresholds()

    def _init_metrics_patterns(self) -> Dict[str, List[str]]:
        """åˆå§‹åŒ–æŒ‡æ ‡æå–æ¨¡å¼"""
        return {
            "loss": [
                r"loss[:=]\s*([0-9]*\.?[0-9]+(?:[eE][-+]?[0-9]+)?)",
                r"train_loss[:=]\s*([0-9]*\.?[0-9]+(?:[eE][-+]?[0-9]+)?)",
                r"validation_loss[:=]\s*([0-9]*\.?[0-9]+(?:[eE][-+]?[0-9]+)?)",
            ],
            "accuracy": [
                r"accuracy[:=]\s*([0-9]*\.?[0-9]+)",
                r"acc[:=]\s*([0-9]*\.?[0-9]+)",
                r"train_acc[:=]\s*([0-9]*\.?[0-9]+)",
                r"val_acc[:=]\s*([0-9]*\.?[0-9]+)",
            ],
            "learning_rate": [
                r"learning_rate[:=]\s*([0-9]*\.?[0-9]+(?:[eE][-+]?[0-9]+)?)",
                r"lr[:=]\s*([0-9]*\.?[0-9]+(?:[eE][-+]?[0-9]+)?)",
            ],
            "gpu_memory": [
                r"GPU.*memory.*([0-9]+\.?[0-9]*)\s*GB",
                r"memory.*usage.*([0-9]+\.?[0-9]*)\s*GB",
                r"allocated.*([0-9]+\.?[0-9]*)\s*[GM]B",
                r"memory usage=([0-9]+\.?[0-9]*)GB",
            ],
            "gpu_utilization": [
                r"GPU.*utilization.*([0-9]+)",
                r"GPU.*usage.*([0-9]+)",
                r"utilization=([0-9]+)%",
            ],
            "throughput": [
                r"throughput[:=]\s*([0-9]*\.?[0-9]+).*tokens?/s",
                r"tokens/second[:=]\s*([0-9]*\.?[0-9]+)",
                r"samples/second[:=]\s*([0-9]*\.?[0-9]+)",
            ],
            "latency": [
                r"latency[:=]\s*([0-9]*\.?[0-9]+).*ms",
                r"inference.*time[:=]\s*([0-9]*\.?[0-9]+).*seconds?",
                r"processing.*time[:=]\s*([0-9]*\.?[0-9]+)",
            ],
            "gradient_norm": [
                r"gradient.*norm[:=]\s*([0-9]*\.?[0-9]+(?:[eE][-+]?[0-9]+)?)",
                r"grad_norm[:=]\s*([0-9]*\.?[0-9]+(?:[eE][-+]?[0-9]+)?)",
            ],
            "epoch": [r"epoch[:=]\s*([0-9]+)", r"Epoch\s+([0-9]+)"],
            "step": [r"step[:=]\s*([0-9]+)", r"Step\s+([0-9]+)"],
        }

    def _init_anomaly_patterns(self) -> Dict[str, List[str]]:
        """åˆå§‹åŒ–å¼‚å¸¸æ£€æµ‹æ¨¡å¼"""
        return {
            "memory_issues": [
                r"out of memory|OOM|OutOfMemoryError",
                r"CUDA out of memory",
                r"memory allocation failed",
                r"insufficient memory",
                r"memory leak",
            ],
            "gpu_errors": [
                r"CUDA error|CUDA runtime error",
                r"CUDNN_STATUS_EXECUTION_FAILED",
                r"GPU device not found",
                r"NVIDIA-SMI has failed",
                r"device-side assert triggered",
            ],
            "training_instability": [
                r"loss.*nan|loss.*inf",
                r"gradient.*nan|gradient.*inf",
                r"gradient.*explosion",
                r"learning rate.*too.*high",
                r"training.*unstable",
                r"diverged|diverging",
            ],
            "convergence_issues": [
                r"no improvement for \d+ epochs",
                r"early stopping",
                r"plateau.*detected",
                r"learning.*stagnated",
            ],
            "data_issues": [
                r"empty.*batch|batch.*size.*0",
                r"data.*corruption|corrupt.*data",
                r"missing.*files|file.*not.*found",
                r"invalid.*data|data.*invalid",
            ],
            "model_issues": [
                r"failed.*load.*model|model.*loading.*failed",
                r"checkpoint.*not.*found|missing.*checkpoint",
                r"model.*format.*error|invalid.*model",
                r"version.*mismatch|incompatible.*version",
            ],
            "performance_issues": [
                r"timeout|time.*out",
                r"slow.*training|training.*slow",
                r"low.*throughput|throughput.*low",
                r"high.*latency|latency.*high",
            ],
        }

    def _init_thresholds(self) -> Dict[str, Dict[str, float]]:
        """åˆå§‹åŒ–å¼‚å¸¸é˜ˆå€¼"""
        return {
            "loss": {"min": 0.0, "max": 100.0, "nan_check": True},
            "accuracy": {"min": 0.0, "max": 1.0},
            "learning_rate": {"min": 1e-8, "max": 1.0},
            "gpu_memory": {"max": 95.0},  # å†…å­˜ä½¿ç”¨ç‡è¶…è¿‡95%
            "gpu_utilization": {"min": 10.0, "max": 100.0},  # åˆ©ç”¨ç‡è¿‡ä½æˆ–è¿‡é«˜
            "throughput": {"min": 1.0},  # ååé‡è¿‡ä½
            "latency": {"max": 10000.0},  # å»¶è¿Ÿè¶…è¿‡10ç§’
            "gradient_norm": {"max": 100.0, "nan_check": True},  # æ¢¯åº¦çˆ†ç‚¸
        }

    def analyze_log_file(self, file_path: str) -> Dict[str, Any]:
        """åˆ†ææ—¥å¿—æ–‡ä»¶"""
        try:
            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                lines = f.readlines()

            metrics = self._extract_metrics(lines)
            anomalies = self._detect_anomalies(lines, metrics)
            summary = self._generate_summary(metrics, anomalies, len(lines))

            return {
                "file_path": file_path,
                "total_lines": len(lines),
                "analysis_time": datetime.now().isoformat(),
                "summary": summary,
                "metrics": self._serialize_metrics(metrics),
                "anomalies": self._serialize_anomalies(anomalies),
                "recommendations": self._generate_recommendations(anomalies, metrics),
            }

        except Exception as e:
            return {"error": f"åˆ†æå¤±è´¥: {str(e)}"}

    def _extract_metrics(self, lines: List[str]) -> Dict[str, List[LogMetric]]:
        """æå–æ—¥å¿—ä¸­çš„æŒ‡æ ‡"""
        metrics: Dict[str, List[LogMetric]] = {
            key: [] for key in self.metrics_patterns.keys()
        }

        for line_num, line in enumerate(lines, 1):
            line = line.strip()
            if not line:
                continue

            # æå–æ—¶é—´æˆ³
            timestamp = self._extract_timestamp(line)

            # æå–å„ç§æŒ‡æ ‡
            for metric_name, patterns in self.metrics_patterns.items():
                for pattern in patterns:
                    matches = re.finditer(pattern, line, re.IGNORECASE)
                    for match in matches:
                        try:
                            value = float(match.group(1))
                            metric = LogMetric(
                                name=metric_name,
                                value=value,
                                timestamp=timestamp,
                                line_number=line_num,
                                context=line,
                            )
                            metrics[metric_name].append(metric)
                        except (ValueError, IndexError):
                            continue

        return metrics

    def _extract_timestamp(self, line: str) -> str:
        """æå–æ—¶é—´æˆ³"""
        # å‡è®¾æ—¶é—´æˆ³æ ¼å¼ä¸º ISO 8601
        match = re.search(r"\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(?:\.\d+)?Z?", line)
        if match:
            return match.group(0)

        return datetime.now().isoformat()  # é»˜è®¤è¿”å›å½“å‰æ—¶é—´

    def _detect_anomalies(
        self, lines: List[str], metrics: Dict[str, List[LogMetric]]
    ) -> List[LogAnomaly]:
        """æ£€æµ‹æ—¥å¿—ä¸­çš„å¼‚å¸¸"""
        anomalies = []

        for line_num, line in enumerate(lines, 1):
            line = line.strip()
            if not line:
                continue

            # æ£€æµ‹å·²çŸ¥çš„å¼‚å¸¸æ¨¡å¼
            for anomaly_category, patterns in self.anomaly_patterns.items():
                for pattern in patterns:
                    if re.search(pattern, line, re.IGNORECASE):
                        anomaly = LogAnomaly(
                            category=anomaly_category,
                            severity=self._determine_severity(anomaly_category),
                            description=f"æ£€æµ‹åˆ°{anomaly_category}å¼‚å¸¸",
                            line_number=line_num,
                            metric_value=None,
                            threshold=None,
                            suggestion=self._generate_suggestion(anomaly_category),
                            context=line,
                        )
                        anomalies.append(anomaly)

        # åŸºäºæŒ‡æ ‡é˜ˆå€¼æ£€æµ‹å¼‚å¸¸
        for metric_name, metric_values in metrics.items():
            if metric_name not in self.thresholds:
                continue

            for metric in metric_values:
                thresholds = self.thresholds[metric_name]

                # æ£€æŸ¥ NaN å’Œ Inf
                if "nan_check" in thresholds and thresholds["nan_check"]:
                    if (
                        metric.value != metric.value
                        or metric.value == float("inf")
                        or metric.value == float("-inf")
                    ):
                        anomaly = LogAnomaly(
                            category=f"{metric_name}_issues",
                            severity="critical",
                            description=f"{metric_name} å€¼å¼‚å¸¸",
                            line_number=metric.line_number,
                            metric_value=metric.value,
                            threshold=None,
                            suggestion="æ£€æŸ¥æ•°æ®æºå’Œé¢„å¤„ç†æ­¥éª¤",
                            context=metric.context,
                        )
                        anomalies.append(anomaly)
                        continue

                # æ£€æŸ¥ä¸Šä¸‹é™
                if "min" in thresholds and metric.value < thresholds["min"]:
                    anomaly = LogAnomaly(
                        category=f"{metric_name}_issues",
                        severity="high",
                        description=f"{metric_name} å€¼ä½äºé˜ˆå€¼",
                        line_number=metric.line_number,
                        metric_value=metric.value,
                        threshold=thresholds["min"],
                        suggestion=f"å¢åŠ {metric_name}ï¼Œå½“å‰å€¼: {metric.value}",
                        context=metric.context,
                    )
                    anomalies.append(anomaly)

                if "max" in thresholds and metric.value > thresholds["max"]:
                    anomaly = LogAnomaly(
                        category=f"{metric_name}_issues",
                        severity="high",
                        description=f"{metric_name} å€¼è¶…è¿‡é˜ˆå€¼",
                        line_number=metric.line_number,
                        metric_value=metric.value,
                        threshold=thresholds["max"],
                        suggestion=f"å‡å°‘{metric_name}ï¼Œå½“å‰å€¼: {metric.value}",
                        context=metric.context,
                    )
                    anomalies.append(anomaly)

        return anomalies

    def _determine_severity(self, anomaly_category: str) -> str:
        """ç¡®å®šå¼‚å¸¸çš„ä¸¥é‡æ€§"""
        if "critical" in anomaly_category:
            return "critical"
        elif "high" in anomaly_category:
            return "high"
        elif "medium" in anomaly_category:
            return "medium"
        else:
            return "low"

    def _generate_summary(
        self,
        metrics: Dict[str, List[LogMetric]],
        anomalies: List[LogAnomaly],
        total_lines: int,
    ) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææ‘˜è¦"""
        summary = {}

        # æŒ‡æ ‡æ‘˜è¦
        for metric_name, metric_values in metrics.items():
            if not metric_values:
                continue

            summary[metric_name] = {
                "count": len(metric_values),
                "min": min(m.value for m in metric_values),
                "max": max(m.value for m in metric_values),
                "avg": sum(m.value for m in metric_values) / len(metric_values),
                "last": metric_values[-1].value,
                "trend": self._detect_trend(metric_values),
            }

        # å¼‚å¸¸æ‘˜è¦
        summary["anomalies"] = {
            "total": len(anomalies),
            "by_category": self._count_anomalies_by_category(anomalies),
            "by_severity": self._count_anomalies_by_severity(anomalies),
        }

        return summary

    def _detect_trend(self, metric_values: List[LogMetric]) -> str:
        """æ£€æµ‹æŒ‡æ ‡è¶‹åŠ¿"""
        if len(metric_values) < 2:
            return "stable"

        # ç®€å•çº¿æ€§å›å½’åˆ¤æ–­è¶‹åŠ¿
        x = list(range(len(metric_values)))
        y = [m.value for m in metric_values]

        # è®¡ç®—æ–œç‡
        slope = (len(x) * sum(xi * yi for xi, yi in zip(x, y)) - sum(x) * sum(y)) / (
            len(x) * sum(xi**2 for xi in x) - sum(x) ** 2
        )

        if slope > 0:
            return "increasing"
        elif slope < 0:
            return "decreasing"
        else:
            return "stable"

    def _count_anomalies_by_category(
        self, anomalies: List[LogAnomaly]
    ) -> Dict[str, int]:
        """æŒ‰ç±»åˆ«ç»Ÿè®¡å¼‚å¸¸"""
        category_counts = {}

        for anomaly in anomalies:
            if anomaly.category not in category_counts:
                category_counts[anomaly.category] = 0
            category_counts[anomaly.category] += 1

        return category_counts

    def _count_anomalies_by_severity(
        self, anomalies: List[LogAnomaly]
    ) -> Dict[str, int]:
        """æŒ‰ä¸¥é‡æ€§ç»Ÿè®¡å¼‚å¸¸"""
        severity_counts = {}

        for anomaly in anomalies:
            if anomaly.severity not in severity_counts:
                severity_counts[anomaly.severity] = 0
            severity_counts[anomaly.severity] += 1

        return severity_counts

    def _serialize_metrics(
        self, metrics: Dict[str, List[LogMetric]]
    ) -> Dict[str, List[Dict[str, Any]]]:
        """åºåˆ—åŒ–æŒ‡æ ‡æ•°æ®"""
        serialized = {
            key: [m.__dict__ for m in value] for key, value in metrics.items()
        }
        return serialized

    def _serialize_anomalies(self, anomalies: List[LogAnomaly]) -> List[Dict[str, Any]]:
        """åºåˆ—åŒ–å¼‚å¸¸æ•°æ®"""
        serialized = [anomaly.__dict__ for anomaly in anomalies]
        return serialized

    def _generate_recommendations(
        self, anomalies: List[LogAnomaly], metrics: Dict[str, List[LogMetric]]
    ) -> List[str]:
        """ç”Ÿæˆé’ˆå¯¹æ€§çš„å»ºè®®"""
        recommendations = []

        for anomaly in anomalies:
            if anomaly.category in ["memory_issues", "gpu_errors"]:
                recommendations.append("æ£€æŸ¥ç¡¬ä»¶èµ„æºï¼Œç¡®ä¿è¶³å¤Ÿçš„å†…å­˜å’ŒGPUå¯ç”¨")

            if anomaly.category in ["training_instability", "convergence_issues"]:
                recommendations.append("è°ƒæ•´å­¦ä¹ ç‡æˆ–å…¶ä»–è¶…å‚æ•°ï¼Œå°è¯•æ¢å¤è®­ç»ƒç¨³å®šæ€§")

            if anomaly.category in ["data_issues"]:
                recommendations.append("æ£€æŸ¥æ•°æ®é›†ï¼Œç¡®ä¿æ•°æ®å®Œæ•´æ€§å’Œæ­£ç¡®æ€§")

            if anomaly.category in ["model_issues"]:
                recommendations.append("æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å’Œè·¯å¾„ï¼Œç¡®ä¿æ¨¡å‹å¯æ­£ç¡®åŠ è½½")

            if anomaly.category in ["performance_issues"]:
                recommendations.append("ä¼˜åŒ–æ¨¡å‹æˆ–æ•°æ®ç®¡é“ï¼Œå°è¯•æé«˜æ€§èƒ½")

        # åŸºäºæŒ‡æ ‡çš„å»ºè®®
        for metric_name, metric_values in metrics.items():
            if metric_name not in self.thresholds:
                continue

            for metric in metric_values:
                thresholds = self.thresholds[metric_name]

                if "min" in thresholds and metric.value < thresholds["min"]:
                    recommendations.append(
                        f"å¢åŠ {metric_name}ï¼Œå½“å‰å€¼è¿‡ä½: {metric.value}"
                    )

                if "max" in thresholds and metric.value > thresholds["max"]:
                    recommendations.append(
                        f"å‡å°‘{metric_name}ï¼Œå½“å‰å€¼è¿‡é«˜: {metric.value}"
                    )

        return list(set(recommendations))  # å»é‡è¿”å›å»ºè®®åˆ—è¡¨

    def _generate_suggestion(self, anomaly_category: str) -> str:
        """ä¸ºç‰¹å®šå¼‚å¸¸ç±»åˆ«ç”Ÿæˆå»ºè®®"""
        suggestions = {
            "memory_issues": "é‡Šæ”¾å†…å­˜ã€å‡å°‘æ‰¹æ¬¡å¤§å°æˆ–å¢åŠ ç³»ç»Ÿå†…å­˜",
            "gpu_errors": "æ£€æŸ¥GPUé©±åŠ¨ã€CUDAç‰ˆæœ¬æˆ–é‡å¯GPUè®¾å¤‡",
            "training_instability": "é™ä½å­¦ä¹ ç‡ã€æ£€æŸ¥æ¢¯åº¦è£å‰ªæˆ–è°ƒæ•´ä¼˜åŒ–å™¨",
            "convergence_issues": "è°ƒæ•´å­¦ä¹ ç‡è°ƒåº¦ã€å¢åŠ è®­ç»ƒè½®æ•°æˆ–æ”¹è¿›æ¨¡å‹æ¶æ„",
            "data_issues": "æ£€æŸ¥æ•°æ®å®Œæ•´æ€§ã€æ¸…ç†æŸåæ•°æ®æˆ–ä¿®å¤æ•°æ®ç®¡é“",
            "model_issues": "éªŒè¯æ¨¡å‹æ–‡ä»¶ã€æ£€æŸ¥ç‰ˆæœ¬å…¼å®¹æ€§æˆ–é‡æ–°ä¸‹è½½æ¨¡å‹",
            "performance_issues": "ä¼˜åŒ–ä»£ç ã€å¢åŠ å¹¶è¡Œåº¦æˆ–å‡çº§ç¡¬ä»¶é…ç½®",
        }
        return suggestions.get(anomaly_category, "æ£€æŸ¥ç›¸å…³é…ç½®å’Œæ—¥å¿—è¯¦æƒ…")

    def create_system_prompt_for_log_analysis(
        self, analysis_result: Dict[str, Any], user_question: Optional[str] = None
    ) -> str:
        """åˆ›å»ºç”¨äºLLMåˆ†æçš„ç³»ç»Ÿæç¤º"""
        summary = analysis_result.get("summary", {})
        anomalies = analysis_result.get("anomalies", [])

        # æ„å»ºç³»ç»Ÿæç¤º
        system_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„AIæ¨¡å‹è®­ç»ƒå’Œæ¨ç†ä¸“å®¶ï¼Œå…·æœ‰ä¸°å¯Œçš„è°ƒè¯•å’Œä¼˜åŒ–ç»éªŒã€‚è¯·åŸºäºä»¥ä¸‹AIæ¨¡å‹æ—¥å¿—åˆ†æç»“æœï¼Œæä¾›ä¸“ä¸šçš„è¯Šæ–­å’Œå»ºè®®ã€‚

## ğŸ“Š æ—¥å¿—åˆ†ææ‘˜è¦

**æ–‡ä»¶ä¿¡æ¯:**
- æ–‡ä»¶è·¯å¾„: {analysis_result.get('file_path', 'N/A')}
- æ€»è¡Œæ•°: {analysis_result.get('total_lines', 0)}
- åˆ†ææ—¶é—´: {analysis_result.get('analysis_time', 'N/A')}

**å¼‚å¸¸ç»Ÿè®¡:**
- æ€»å¼‚å¸¸æ•°: {summary.get('anomalies', {}).get('total', 0)}

"""

        # æ·»åŠ ä¸¥é‡ç¨‹åº¦ç»Ÿè®¡
        if "by_severity" in summary.get("anomalies", {}):
            system_prompt += "**æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ:**\n"
            for severity, count in summary["anomalies"]["by_severity"].items():
                system_prompt += f"- {severity}: {count}ä¸ª\n"
            system_prompt += "\n"

        # æ·»åŠ ç±»åˆ«ç»Ÿè®¡
        if "by_category" in summary.get("anomalies", {}):
            system_prompt += "**æŒ‰ç±»åˆ«åˆ†å¸ƒ:**\n"
            for category, count in summary["anomalies"]["by_category"].items():
                system_prompt += f"- {category}: {count}ä¸ª\n"
            system_prompt += "\n"

        # æ·»åŠ å…³é”®æŒ‡æ ‡
        system_prompt += "## ğŸ“ˆ å…³é”®æŒ‡æ ‡åˆ†æ\n\n"
        metrics_added = 0
        for metric_name, metric_data in summary.items():
            if (
                metric_name == "anomalies"
                or not isinstance(metric_data, dict)
                or "count" not in metric_data
            ):
                continue
            if metric_data.get("count", 0) > 0:
                system_prompt += f"**{metric_name}:**\n"
                system_prompt += f"- æ•°æ®ç‚¹æ•°é‡: {metric_data['count']}\n"
                system_prompt += (
                    f"- æ•°å€¼èŒƒå›´: {metric_data['min']:.4f} - {metric_data['max']:.4f}\n"
                )
                system_prompt += f"- å¹³å‡å€¼: {metric_data['avg']:.4f}\n"
                system_prompt += f"- æœ€æ–°å€¼: {metric_data['last']:.4f}\n"
                system_prompt += f"- è¶‹åŠ¿: {metric_data.get('trend', 'unknown')}\n\n"
                metrics_added += 1
                if metrics_added >= 8:  # é™åˆ¶æŒ‡æ ‡æ•°é‡
                    break

        # æ·»åŠ è¯¦ç»†å¼‚å¸¸ä¿¡æ¯
        if anomalies:
            system_prompt += "## âš ï¸ æ£€æµ‹åˆ°çš„å¼‚å¸¸è¯¦æƒ…\n\n"

            # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åºå¼‚å¸¸
            critical_anomalies = [
                a for a in anomalies if a.get("severity") == "critical"
            ]
            high_anomalies = [a for a in anomalies if a.get("severity") == "high"]
            medium_anomalies = [a for a in anomalies if a.get("severity") == "medium"]

            # æ˜¾ç¤ºå…³é”®å¼‚å¸¸
            if critical_anomalies:
                system_prompt += "**ğŸ”´ ä¸¥é‡å¼‚å¸¸:**\n"
                for i, anomaly in enumerate(critical_anomalies[:5], 1):  # æœ€å¤š5ä¸ª
                    system_prompt += f"{i}. {anomaly.get('description', 'N/A')} (è¡Œ {anomaly.get('line_number', 'N/A')})\n"
                    system_prompt += f"   ç±»åˆ«: {anomaly.get('category', 'N/A')}\n"
                    if anomaly.get("metric_value") is not None:
                        system_prompt += f"   æŒ‡æ ‡å€¼: {anomaly.get('metric_value')}\n"
                    if anomaly.get("threshold") is not None:
                        system_prompt += f"   é˜ˆå€¼: {anomaly.get('threshold')}\n"
                    system_prompt += (
                        f"   ä¸Šä¸‹æ–‡: {anomaly.get('context', 'N/A')[:100]}...\n\n"
                    )

            # æ˜¾ç¤ºé«˜çº§å¼‚å¸¸
            if high_anomalies:
                system_prompt += "**ğŸŸ  é«˜çº§å¼‚å¸¸:**\n"
                for i, anomaly in enumerate(high_anomalies[:3], 1):  # æœ€å¤š3ä¸ª
                    system_prompt += f"{i}. {anomaly.get('description', 'N/A')} (è¡Œ {anomaly.get('line_number', 'N/A')})\n"
                    system_prompt += f"   ç±»åˆ«: {anomaly.get('category', 'N/A')}\n"
                    if anomaly.get("metric_value") is not None:
                        system_prompt += f"   æŒ‡æ ‡å€¼: {anomaly.get('metric_value')}\n"
                    system_prompt += (
                        f"   ä¸Šä¸‹æ–‡: {anomaly.get('context', 'N/A')[:100]}...\n\n"
                    )

            # æ˜¾ç¤ºä¸­ç­‰å¼‚å¸¸ï¼ˆç®€åŒ–ç‰ˆï¼‰
            if medium_anomalies:
                system_prompt += f"**ğŸŸ¡ ä¸­ç­‰å¼‚å¸¸:** {len(medium_anomalies)}ä¸ª\n"
                for anomaly in medium_anomalies[:2]:
                    system_prompt += f"- {anomaly.get('description', 'N/A')} (è¡Œ {anomaly.get('line_number', 'N/A')})\n"
                system_prompt += "\n"

        # æ·»åŠ ç”¨æˆ·ç‰¹å®šé—®é¢˜
        if user_question:
            system_prompt += (
                f"## ğŸ¯ ç”¨æˆ·å…³æ³¨çš„å…·ä½“é—®é¢˜\n\n**é—®é¢˜:** {user_question}\n\n"
            )

        # æ·»åŠ åˆ†ææŒ‡å¯¼
        system_prompt += """## ğŸ“‹ è¯·æä¾›ä»¥ä¸‹åˆ†æ

è¯·ä½œä¸ºAIæ¨¡å‹ä¸“å®¶ï¼ŒåŸºäºä¸Šè¿°æ—¥å¿—åˆ†æç»“æœï¼Œæä¾›ï¼š

1. **ğŸ” é—®é¢˜è¯Šæ–­**: 
   - è¯†åˆ«æœ€å…³é”®çš„é—®é¢˜å’Œé£é™©
   - åˆ†æé—®é¢˜çš„æ ¹æœ¬åŸå› 
   - è¯„ä¼°é—®é¢˜çš„ä¸¥é‡ç¨‹åº¦å’Œå½±å“

2. **ğŸ› ï¸ è§£å†³æ–¹æ¡ˆ**: 
   - æä¾›å…·ä½“çš„ä¿®å¤å»ºè®®
   - æ¨èæœ€ä½³å®è·µ
   - ç»™å‡ºä¼˜å…ˆçº§æ’åº

3. **ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®**: 
   - åŸºäºæŒ‡æ ‡è¶‹åŠ¿çš„ä¼˜åŒ–å»ºè®®
   - èµ„æºé…ç½®ä¼˜åŒ–
   - è®­ç»ƒ/æ¨ç†æ•ˆç‡æå‡

4. **ğŸš¨ é¢„é˜²æªæ–½**: 
   - å¦‚ä½•é¿å…ç±»ä¼¼é—®é¢˜
   - ç›‘æ§å’Œé¢„è­¦å»ºè®®
   - é…ç½®ä¼˜åŒ–å»ºè®®

è¯·ç”¨ä¸“ä¸šã€æ¸…æ™°ã€æ˜“æ‡‚çš„è¯­è¨€å›ç­”ï¼Œå¹¶æä¾›å¯æ‰§è¡Œçš„å…·ä½“æ­¥éª¤ã€‚å¦‚æœæ—¥å¿—ä¸­æ²¡æœ‰æ˜æ˜¾é—®é¢˜ï¼Œè¯·åˆ†ææ•´ä½“å¥åº·çŠ¶å†µå¹¶æä¾›ä¼˜åŒ–å»ºè®®ã€‚
"""

        return system_prompt

    def analyze_with_llm_assistance(
        self,
        file_path: str,
        user_question: Optional[str] = None,
        llm_model: str = "deepseek-chat",
    ) -> Dict[str, Any]:
        """ä½¿ç”¨LLMè¾…åŠ©è¿›è¡Œæ™ºèƒ½æ—¥å¿—åˆ†æ"""
        # é¦–å…ˆè¿›è¡ŒåŸºç¡€åˆ†æ
        basic_analysis = self.analyze_log_file(file_path)

        if "error" in basic_analysis:
            return basic_analysis

        # åˆ›å»ºç³»ç»Ÿæç¤º
        system_prompt = self.create_system_prompt_for_log_analysis(
            basic_analysis, user_question
        )

        # æ„å»ºç”¨æˆ·æç¤º
        user_prompt = "è¯·åˆ†æè¿™ä¸ªAIæ¨¡å‹æ—¥å¿—å¹¶æä¾›ä¸“ä¸šçš„è¯Šæ–­å’Œå»ºè®®ã€‚"
        if user_question:
            user_prompt += f"\n\nç‰¹åˆ«å…³æ³¨è¿™ä¸ªé—®é¢˜: {user_question}"

        # è¿”å›å®Œæ•´åˆ†æç»“æœ
        return {
            **basic_analysis,
            "llm_analysis": {
                "system_prompt": system_prompt,
                "user_prompt": user_prompt,
                "model": llm_model,
                "ready_for_llm": True,
            },
        }
