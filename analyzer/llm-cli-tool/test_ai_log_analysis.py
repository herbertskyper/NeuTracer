#!/usr/bin/env python3
"""
AIæ¨¡å‹æ—¥å¿—åˆ†æåŠŸèƒ½æµ‹è¯•è„šæœ¬
"""

import os
import sys

sys.path.append("src")

from model_log_analyzer import ModelLogAnalyzer


def test_basic_analysis():
    """æµ‹è¯•åŸºç¡€åˆ†æåŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•åŸºç¡€æ—¥å¿—åˆ†æ...")

    analyzer = ModelLogAnalyzer()
    result = analyzer.analyze_log_file("example_training_log.txt")

    assert "error" not in result, f"åˆ†æå¤±è´¥: {result.get('error')}"
    assert "summary" in result, "ç¼ºå°‘æ‘˜è¦"
    assert "anomalies" in result, "ç¼ºå°‘å¼‚å¸¸æ£€æµ‹"
    assert "metrics" in result, "ç¼ºå°‘æŒ‡æ ‡æå–"

    anomalies = result["anomalies"]
    summary = result["summary"]

    print(f"âœ… æ£€æµ‹åˆ° {len(anomalies)} ä¸ªå¼‚å¸¸")
    print(f"âœ… å¼‚å¸¸ç»Ÿè®¡: {summary.get('anomalies', {})}")

    return True


def test_llm_prompt_generation():
    """æµ‹è¯•LLMç³»ç»Ÿæç¤ºç”Ÿæˆ"""
    print("\nğŸ§ª æµ‹è¯•LLMç³»ç»Ÿæç¤ºç”Ÿæˆ...")

    analyzer = ModelLogAnalyzer()
    result = analyzer.analyze_with_llm_assistance(
        "example_training_log.txt", "GPUå†…å­˜ä½¿ç”¨æ˜¯å¦æ­£å¸¸ï¼Ÿ"
    )

    assert "error" not in result, f"åˆ†æå¤±è´¥: {result.get('error')}"
    assert "llm_analysis" in result, "ç¼ºå°‘LLMåˆ†ææ•°æ®"

    llm_data = result["llm_analysis"]
    assert "system_prompt" in llm_data, "ç¼ºå°‘ç³»ç»Ÿæç¤º"
    assert "ready_for_llm" in llm_data, "LLMå‡†å¤‡çŠ¶æ€ç¼ºå¤±"
    assert llm_data["ready_for_llm"], "LLMæœªå‡†å¤‡å°±ç»ª"

    system_prompt = llm_data["system_prompt"]
    print(f"âœ… ç”Ÿæˆç³»ç»Ÿæç¤ºé•¿åº¦: {len(system_prompt)} å­—ç¬¦")
    print(f"âœ… LLMå‡†å¤‡çŠ¶æ€: {llm_data['ready_for_llm']}")

    return True


def test_anomaly_detection():
    """æµ‹è¯•å¼‚å¸¸æ£€æµ‹åŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•å¼‚å¸¸æ£€æµ‹åŠŸèƒ½...")

    analyzer = ModelLogAnalyzer()
    result = analyzer.analyze_log_file("example_inference_log.txt")

    assert "error" not in result, f"åˆ†æå¤±è´¥: {result.get('error')}"

    anomalies = result["anomalies"]
    by_severity = result["summary"]["anomalies"]["by_severity"]
    by_category = result["summary"]["anomalies"]["by_category"]

    print(f"âœ… æ£€æµ‹åˆ° {len(anomalies)} ä¸ªå¼‚å¸¸")
    print(f"âœ… æŒ‰ä¸¥é‡ç¨‹åº¦: {by_severity}")
    print(f"âœ… æŒ‰ç±»åˆ«: {by_category}")

    # éªŒè¯å¼‚å¸¸æ•°æ®ç»“æ„
    if anomalies:
        first_anomaly = anomalies[0]
        required_fields = [
            "category",
            "severity",
            "description",
            "line_number",
            "suggestion",
            "context",
        ]
        for field in required_fields:
            assert field in first_anomaly, f"å¼‚å¸¸æ•°æ®ç¼ºå°‘å­—æ®µ: {field}"

    return True


def test_metrics_extraction():
    """æµ‹è¯•æŒ‡æ ‡æå–åŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•æŒ‡æ ‡æå–åŠŸèƒ½...")

    analyzer = ModelLogAnalyzer()
    result = analyzer.analyze_log_file("example_training_log.txt")

    assert "error" not in result, f"åˆ†æå¤±è´¥: {result.get('error')}"

    metrics = result["metrics"]
    summary = result["summary"]

    # éªŒè¯æŒ‡æ ‡æå–
    expected_metrics = ["loss", "accuracy", "learning_rate", "gpu_memory"]
    extracted_metrics = []

    for metric_name in expected_metrics:
        if metric_name in metrics and metrics[metric_name]:
            extracted_metrics.append(metric_name)
            print(f"âœ… æå–åˆ° {metric_name}: {len(metrics[metric_name])} ä¸ªæ•°æ®ç‚¹")

    assert len(extracted_metrics) > 0, "æœªæå–åˆ°ä»»ä½•é¢„æœŸæŒ‡æ ‡"

    # éªŒè¯è¶‹åŠ¿åˆ†æ
    for metric_name in extracted_metrics:
        if metric_name in summary:
            trend = summary[metric_name].get("trend", "unknown")
            print(f"âœ… {metric_name} è¶‹åŠ¿: {trend}")

    return True


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹AIæ¨¡å‹æ—¥å¿—åˆ†æåŠŸèƒ½æµ‹è¯•")
    print("=" * 50)

    # æ£€æŸ¥æµ‹è¯•æ–‡ä»¶
    test_files = ["example_training_log.txt", "example_inference_log.txt"]
    for file in test_files:
        if not os.path.exists(file):
            print(f"âŒ æµ‹è¯•æ–‡ä»¶ç¼ºå¤±: {file}")
            return False

    tests = [
        test_basic_analysis,
        test_anomaly_detection,
        test_metrics_extraction,
        test_llm_prompt_generation,
    ]

    passed = 0
    failed = 0

    for test_func in tests:
        try:
            if test_func():
                passed += 1
                print(f"âœ… {test_func.__name__} é€šè¿‡")
            else:
                failed += 1
                print(f"âŒ {test_func.__name__} å¤±è´¥")
        except Exception as e:
            failed += 1
            print(f"âŒ {test_func.__name__} å¼‚å¸¸: {str(e)}")

    print("\n" + "=" * 50)
    print(f"ğŸ¯ æµ‹è¯•ç»“æœ: {passed} é€šè¿‡, {failed} å¤±è´¥")

    if failed == 0:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼AIæ¨¡å‹æ—¥å¿—åˆ†æåŠŸèƒ½æ­£å¸¸å·¥ä½œ")
        return True
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³åŠŸèƒ½")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
